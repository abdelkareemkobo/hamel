{"title":"axolotl start here","markdown":{"yaml":{"title":"axolotl start here","description":"Best practices for debugging axolotl with an example VSCode config.","categories":["LLMs","fine-tuning","axolotl"],"author":"Hamel Husain","date":"2024-01-11","image":"images/debug_axolotl_small.png","order":1},"headingText":"Motivation","containsRefs":false,"markdown":"\n\n![](images/debug_axolotl.png)\n\n\n[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) is a great project for fine-tuning LLMs.  I started contributing to the project, and I found that it was difficult to debug.  I wanted to share some tips and tricks I learned along the way, along with configuration files for debugging with VSCode.  Moreover, I think being able to debug axolotl empowers developers who encounter bugs or want to understand how the code works.  I hope this document helps you get started.\n\n:::{.callout-important}\n\n## This content is now part of the Axolotl docs!\n\n[I contributed](https://github.com/OpenAccess-AI-Collective/axolotl/pull/1089) this blog post's contents as documentation for the axolotl project. **You can find this content [in the axolotl repo here](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/debugging.md)**.\n\n:::\n\n## General Tips\n\nWhile debugging, it's helpful to simplify your test scenario as much as possible.  Here are some tips for doing so:\n\n:::{.callout-note}\nAll of these tips are incorporated into the [example configuration](#configuration) for debugging with VSCode below.\n:::\n\n1. **Make sure you are using the latest version of axolotl**:  This project changes often and bugs get fixed fast.  Check your git branch and make sure you have pulled the latest changes from `main`.\n2. **Eliminate Concurrency**: Restrict the number of processes to 1 for both training and data preprocessing:\n    - Set `CUDA_VISIBLE_DEVICES` to a single GPU, ex: `export CUDA_VISIBLE_DEVICES=0`.\n    - Set `dataset_processes: 1` in your axolotl config or run the training command with `--dataset_processes=1`.\n3. **Use a small dataset**: Construct or use a small dataset from HF Hub. When using a small dataset, you will often have to make sure `sample_packing: False` and `eval_sample_packing: False` to avoid errors.  If you are in a pinch and don't have time to construct a small dataset but want to use from the HF Hub, you can shard the data (this will still tokenize the entire dataset but will only use a fraction of the data for training.  For example, to shard the dataset into 20 pieces, add the following to your axolotl config):\n\n    ```yaml\n    dataset:\n        ...\n        shards: 20\n    ```\n\n4. **Use a small model**: A good example of a small model is [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0).\n5. **Minimize iteration time**: Make sure the training loop finishes as fast as possible, with these settings.\n    - `micro_batch_size: 1`\n    - `max_steps: 1` \n    - `val_set_size: 0`\n6. **Clear Caches:** Axolotl caches certain steps and so does the underlying HuggingFace trainer.  You may want to clear some of these caches when debugging.\n    - Data preprocessing: When debugging data preprocessing, which includes prompt template formation, you may want to delete the directory set in `dataset_prepared_path:` in your axolotl config.  If you didn't set this value, the default is `last_run_prepared`.\n    - HF Hub: If you are debugging data preprocessing, you should clear the relevant HF cache [HuggingFace cache](https://huggingface.co/docs/datasets/cache), by deleting the appropriate `~/.cache/huggingface/datasets/...` folder(s).\n    - **The recommended approach is to redirect all outputs and caches to a temporary folder and delete selected subfolders before each run.  This is demonstrated in the example configuration below.**\n        \n\n## Debugging with VSCode\n\n### Background\n\nThe below example shows how to configure VSCode to debug data preprocessing of the `sharegpt` format.  This is the format used when you have the following in your axolotl config:\n\n```yaml\ndatasets:\n  - path: <path to your sharegpt formatted dataset> # example on HF Hub: philschmid/guanaco-sharegpt-style\n    type: sharegpt\n```\n\n:::{.callout-important}\nIf you are already familiar with advanced VSCode debugging, you can skip the below explanation and look at the files [.vscode/launch.json](../.vscode/launch.json) and [.vscode/tasks.json](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/tasks.json) for an example configuration.\n:::\n\n:::{.callout-tip}\nIf you prefer to watch a video, rather than read, you can skip to the [video tutorial](#video-tutorial) below (but doing both is recommended).\n:::\n\n### Setup\n\nMake sure you have an [editable install](https://setuptools.pypa.io/en/latest/userguide/development_mode.html) of Axolotl, which ensures that changes you make to the code are reflected at runtime.  Run the following commands from the root of this project:\n\n```bash\npip3 install packaging\npip3 install -e '.[flash-attn,deepspeed]'\n```\n\n#### Remote Hosts\n\nIf you developing on a remote host, you can easily use VSCode to debug remotely.  To do so, you will need to follow this [remote - SSH guide](https://code.visualstudio.com/docs/remote/ssh).  You can also see the video below on [Docker and Remote SSH debugging](#video---attaching-to-docker-on-remote-host).\n\n### Configuration\n\nThe easiest way to get started is to modify the [.vscode/launch.json](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/launch.json) file in the axolotl GitHub repo.  This is just an example configuration, so you may need to modify or copy it to suit your needs.\n\nFor example, to mimic the command `cd devtools && CUDA_VISIBLE_DEVICES=0 accelerate launch -m axolotl.cli.train dev_sharegpt.yml`, you would use the below configuration[^1].  Note that we add additional flags that override the axolotl config and incorporate the tips above (see the comments). We also set the working directory to `devtools` and set the `env` variable `HF_HOME` to a temporary folder that is later partially deleted.  This is because we want to delete the HF dataset cache before each run in order to ensure that the data preprocessing code is run from scratch.\n\n```js\n// https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/launch.json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Debug axolotl prompt - sharegpt\",\n            \"type\": \"python\",\n            \"module\": \"accelerate.commands.launch\",\n            \"request\": \"launch\",\n            \"args\": [\n                \"-m\", \"axolotl.cli.train\", \"dev_sharegpt.yml\",\n                // The flags below simplify debugging by overriding the axolotl config \n                // with the debugging tips above.  Modify as needed.\n                \"--dataset_processes=1\",      // limits data preprocessing to one process\n                \"--max_steps=1\",              // limits training to just one step\n                \"--batch_size=1\",             // minimizes batch size\n                \"--micro_batch_size=1\",       // minimizes batch size\n                \"--val_set_size=0\",           // disables validation\n                \"--sample_packing=False\",     // disables sample packing which is necessary for small datasets\n                \"--eval_sample_packing=False\",// disables sample packing on eval set\n                \"--dataset_prepared_path=temp_debug/axolotl_outputs/data\", // send data outputs to a temp folder\n                \"--output_dir=temp_debug/axolotl_outputs/model\" // send model outputs to a temp folder\n                ],\n            \"console\": \"integratedTerminal\",      // show output in the integrated terminal\n            \"cwd\": \"${workspaceFolder}/devtools\", // set working directory to devtools from the root of the project\n            \"justMyCode\": true,                   // step through only axolotl code\n            \"env\": {\"CUDA_VISIBLE_DEVICES\": \"0\",  // Since we aren't doing distributed training, we need to limit to one GPU\n                    \"HF_HOME\": \"${workspaceFolder}/devtools/temp_debug/.hf-cache\"}, // send HF cache to a temp folder\n            \"preLaunchTask\": \"cleanup-for-dataprep\", // delete temp folders (see below)\n        }\n    ]\n}\n```\n\n**Additional notes about this configuration:**\n\n- The argument `justMyCode` is set to `true` such that you step through only the axolotl code.  If you want to step into dependencies, set this to `false`.\n- The `preLaunchTask`: `cleanup-for-dataprep` is defined in [.vscode/tasks.json](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/tasks.json) and is used to delete the following folders before debugging, which is essential to ensure that the data pre-processing code is run from scratch:\n    -  `./devtools/temp_debug/axolotl_outputs` \n    - `./devtools/temp_debug/.hf-cache/datasets`\n\n:::{.callout-tip}\n\nYou may not want to delete these folders. For example, if you are debugging model training instead of data pre-processing, you may NOT want to delete the cache or output folders. You may also need to add additional tasks to the `tasks.json` file depending on your use case.\n\n:::\n\nBelow is the [./vscode/tasks.json](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/tasks.json) file that defines the `cleanup-for-dataprep` task.  This task is run before each debugging session when you use the above configuration.  Note how there are two tasks that delete the two folders mentioned above.  The third task `cleanup-for-dataprep` is a composite task that combines the two tasks.  A composite task is necessary because VSCode does not allow you to specify multiple tasks in the `preLaunchTask` argument of the `launch.json` file.\n\n```js\n// https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/tasks.json\n// this file is used by launch.json\n{\n    \"version\": \"2.0.0\",\n    \"tasks\": [\n      // this task changes into the devtools directory and deletes the temp_debug/axolotl_outputs folder\n      {\n        \"label\": \"delete-outputs\",\n        \"type\": \"shell\",\n        \"command\": \"rm -rf temp_debug/axolotl_outputs\",\n        \"options\":{ \"cwd\": \"${workspaceFolder}/devtools\"},\n        \"problemMatcher\": []\n      },\n      // this task changes into the devtools directory and deletes the `temp_debug/.hf-cache/datasets` folder\n      {\n        \"label\": \"delete-temp-hf-dataset-cache\",\n        \"type\": \"shell\",\n        \"command\": \"rm -rf temp_debug/.hf-cache/datasets\",\n        \"options\":{ \"cwd\": \"${workspaceFolder}/devtools\"},\n        \"problemMatcher\": []\n      },\n        // this task combines the two tasks above\n      {\n       \"label\": \"cleanup-for-dataprep\",\n       \"dependsOn\": [\"delete-outputs\", \"delete-temp-hf-dataset-cache\"],\n      }\n    ]\n}\n```\n\n### Customizing your debugger\n\nYour debugging use case may differ from the example above.  The easiest thing to do is to put your own axolotl config in the `devtools` folder and modify the `launch.json` file to use your config.  You may also want to modify the `preLaunchTask` to delete different folders or not delete anything at all.\n\n### Video Tutorial\n\nThe following video tutorial walks through the above configuration and demonstrates how to debug with VSCode:\n\n{{< video https://youtu.be/xUUB11yeMmc >}}\n\n\n## Debugging With Docker\n\nUsing [official Axolotl Docker images](https://hub.docker.com/r/winglian/axolotl/tags) is a great way to debug your code, and is a very popular way to use Axolotl.  Attaching VSCode to Docker takes a few more steps.  \n\n### Setup\n\nOn the host that is running axolotl (ex: if you are using a remote host), clone the axolotl repo and change your current directory to the root:\n\n```bash\ngit clone https://github.com/OpenAccess-AI-Collective/axolotl\ncd axolotl\n```\n\n:::{.callout-tip}\nIf you already have axolotl cloned on your host, make sure you have the latest changes and change into the root of the project.\n:::\n\nNext, run the desired docker image and mount the current directory. Below is a docker command you can run to do this:[^2]\n\n```bash\ndocker run --privileged --gpus '\"all\"' --shm-size 10g --rm -it --name axolotl --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --mount type=bind,src=\"${PWD}\",target=/workspace/axolotl -v ${HOME}/.cache/huggingface:/root/.cache/huggingface winglian/axolotl:main-py3.10-cu118-2.0.1\n```\n\n:::{.callout-tip}\nTo understand which containers are available, see the [Docker section of the README](../README.md#docker) and the [DockerHub repo](https://hub.docker.com/r/winglian/axolotl/tags).  For details of how the Docker containers are built, see axolotl's [Docker CI builds](../.github/workflows/main.yml).\n:::\n\nYou will now be in the container.  Next, perform an editable install of Axolotl:\n\n```bash\npip3 install packaging\npip3 install -e '.[flash-attn,deepspeed]'\n```\n\n### Attach To Container\n\nNext, if you are using a remote host, [Remote into this host with VSCode](https://code.visualstudio.com/docs/remote/ssh).  If you are using a local host, you can skip this step.\n\nNext, select `Dev Containers: Attach to Running Container...` using the command palette (`CMD + SHIFT + P`) in VSCode.  You will be prompted to select a container to attach to.  Select the container you just created.  You will now be in the container with a working directory that is at the root of the project.  Any changes you make to the code will be reflected both in the container and on the host.\n\nNow you are ready to debug as described above (see [Debugging with VSCode](#debugging-with-vscode)). \n\n### Video - Attaching To Docker On Remote Host\n\nHere is a short video that demonstrates how to attach to a Docker container on a remote host:\n\n{{< video https://youtu.be/0AuoR7QnHR0 >}}\n\n[^1]: The config actually mimics the command `CUDA_VISIBLE_DEVICES=0 python -m accelerate.commands.launch -m axolotl.cli.train devtools/sharegpt.yml`, but this is the same thing.\n\n[^2]: Many of the below flags are recommended best practices by Nvidia when using nvidia-container-toolkit.  You can read more about these flags [here](https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html).","srcMarkdownNoYaml":"\n\n![](images/debug_axolotl.png)\n\n## Motivation\n\n[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) is a great project for fine-tuning LLMs.  I started contributing to the project, and I found that it was difficult to debug.  I wanted to share some tips and tricks I learned along the way, along with configuration files for debugging with VSCode.  Moreover, I think being able to debug axolotl empowers developers who encounter bugs or want to understand how the code works.  I hope this document helps you get started.\n\n:::{.callout-important}\n\n## This content is now part of the Axolotl docs!\n\n[I contributed](https://github.com/OpenAccess-AI-Collective/axolotl/pull/1089) this blog post's contents as documentation for the axolotl project. **You can find this content [in the axolotl repo here](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/debugging.md)**.\n\n:::\n\n## General Tips\n\nWhile debugging, it's helpful to simplify your test scenario as much as possible.  Here are some tips for doing so:\n\n:::{.callout-note}\nAll of these tips are incorporated into the [example configuration](#configuration) for debugging with VSCode below.\n:::\n\n1. **Make sure you are using the latest version of axolotl**:  This project changes often and bugs get fixed fast.  Check your git branch and make sure you have pulled the latest changes from `main`.\n2. **Eliminate Concurrency**: Restrict the number of processes to 1 for both training and data preprocessing:\n    - Set `CUDA_VISIBLE_DEVICES` to a single GPU, ex: `export CUDA_VISIBLE_DEVICES=0`.\n    - Set `dataset_processes: 1` in your axolotl config or run the training command with `--dataset_processes=1`.\n3. **Use a small dataset**: Construct or use a small dataset from HF Hub. When using a small dataset, you will often have to make sure `sample_packing: False` and `eval_sample_packing: False` to avoid errors.  If you are in a pinch and don't have time to construct a small dataset but want to use from the HF Hub, you can shard the data (this will still tokenize the entire dataset but will only use a fraction of the data for training.  For example, to shard the dataset into 20 pieces, add the following to your axolotl config):\n\n    ```yaml\n    dataset:\n        ...\n        shards: 20\n    ```\n\n4. **Use a small model**: A good example of a small model is [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0).\n5. **Minimize iteration time**: Make sure the training loop finishes as fast as possible, with these settings.\n    - `micro_batch_size: 1`\n    - `max_steps: 1` \n    - `val_set_size: 0`\n6. **Clear Caches:** Axolotl caches certain steps and so does the underlying HuggingFace trainer.  You may want to clear some of these caches when debugging.\n    - Data preprocessing: When debugging data preprocessing, which includes prompt template formation, you may want to delete the directory set in `dataset_prepared_path:` in your axolotl config.  If you didn't set this value, the default is `last_run_prepared`.\n    - HF Hub: If you are debugging data preprocessing, you should clear the relevant HF cache [HuggingFace cache](https://huggingface.co/docs/datasets/cache), by deleting the appropriate `~/.cache/huggingface/datasets/...` folder(s).\n    - **The recommended approach is to redirect all outputs and caches to a temporary folder and delete selected subfolders before each run.  This is demonstrated in the example configuration below.**\n        \n\n## Debugging with VSCode\n\n### Background\n\nThe below example shows how to configure VSCode to debug data preprocessing of the `sharegpt` format.  This is the format used when you have the following in your axolotl config:\n\n```yaml\ndatasets:\n  - path: <path to your sharegpt formatted dataset> # example on HF Hub: philschmid/guanaco-sharegpt-style\n    type: sharegpt\n```\n\n:::{.callout-important}\nIf you are already familiar with advanced VSCode debugging, you can skip the below explanation and look at the files [.vscode/launch.json](../.vscode/launch.json) and [.vscode/tasks.json](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/tasks.json) for an example configuration.\n:::\n\n:::{.callout-tip}\nIf you prefer to watch a video, rather than read, you can skip to the [video tutorial](#video-tutorial) below (but doing both is recommended).\n:::\n\n### Setup\n\nMake sure you have an [editable install](https://setuptools.pypa.io/en/latest/userguide/development_mode.html) of Axolotl, which ensures that changes you make to the code are reflected at runtime.  Run the following commands from the root of this project:\n\n```bash\npip3 install packaging\npip3 install -e '.[flash-attn,deepspeed]'\n```\n\n#### Remote Hosts\n\nIf you developing on a remote host, you can easily use VSCode to debug remotely.  To do so, you will need to follow this [remote - SSH guide](https://code.visualstudio.com/docs/remote/ssh).  You can also see the video below on [Docker and Remote SSH debugging](#video---attaching-to-docker-on-remote-host).\n\n### Configuration\n\nThe easiest way to get started is to modify the [.vscode/launch.json](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/launch.json) file in the axolotl GitHub repo.  This is just an example configuration, so you may need to modify or copy it to suit your needs.\n\nFor example, to mimic the command `cd devtools && CUDA_VISIBLE_DEVICES=0 accelerate launch -m axolotl.cli.train dev_sharegpt.yml`, you would use the below configuration[^1].  Note that we add additional flags that override the axolotl config and incorporate the tips above (see the comments). We also set the working directory to `devtools` and set the `env` variable `HF_HOME` to a temporary folder that is later partially deleted.  This is because we want to delete the HF dataset cache before each run in order to ensure that the data preprocessing code is run from scratch.\n\n```js\n// https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/launch.json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Debug axolotl prompt - sharegpt\",\n            \"type\": \"python\",\n            \"module\": \"accelerate.commands.launch\",\n            \"request\": \"launch\",\n            \"args\": [\n                \"-m\", \"axolotl.cli.train\", \"dev_sharegpt.yml\",\n                // The flags below simplify debugging by overriding the axolotl config \n                // with the debugging tips above.  Modify as needed.\n                \"--dataset_processes=1\",      // limits data preprocessing to one process\n                \"--max_steps=1\",              // limits training to just one step\n                \"--batch_size=1\",             // minimizes batch size\n                \"--micro_batch_size=1\",       // minimizes batch size\n                \"--val_set_size=0\",           // disables validation\n                \"--sample_packing=False\",     // disables sample packing which is necessary for small datasets\n                \"--eval_sample_packing=False\",// disables sample packing on eval set\n                \"--dataset_prepared_path=temp_debug/axolotl_outputs/data\", // send data outputs to a temp folder\n                \"--output_dir=temp_debug/axolotl_outputs/model\" // send model outputs to a temp folder\n                ],\n            \"console\": \"integratedTerminal\",      // show output in the integrated terminal\n            \"cwd\": \"${workspaceFolder}/devtools\", // set working directory to devtools from the root of the project\n            \"justMyCode\": true,                   // step through only axolotl code\n            \"env\": {\"CUDA_VISIBLE_DEVICES\": \"0\",  // Since we aren't doing distributed training, we need to limit to one GPU\n                    \"HF_HOME\": \"${workspaceFolder}/devtools/temp_debug/.hf-cache\"}, // send HF cache to a temp folder\n            \"preLaunchTask\": \"cleanup-for-dataprep\", // delete temp folders (see below)\n        }\n    ]\n}\n```\n\n**Additional notes about this configuration:**\n\n- The argument `justMyCode` is set to `true` such that you step through only the axolotl code.  If you want to step into dependencies, set this to `false`.\n- The `preLaunchTask`: `cleanup-for-dataprep` is defined in [.vscode/tasks.json](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/tasks.json) and is used to delete the following folders before debugging, which is essential to ensure that the data pre-processing code is run from scratch:\n    -  `./devtools/temp_debug/axolotl_outputs` \n    - `./devtools/temp_debug/.hf-cache/datasets`\n\n:::{.callout-tip}\n\nYou may not want to delete these folders. For example, if you are debugging model training instead of data pre-processing, you may NOT want to delete the cache or output folders. You may also need to add additional tasks to the `tasks.json` file depending on your use case.\n\n:::\n\nBelow is the [./vscode/tasks.json](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/tasks.json) file that defines the `cleanup-for-dataprep` task.  This task is run before each debugging session when you use the above configuration.  Note how there are two tasks that delete the two folders mentioned above.  The third task `cleanup-for-dataprep` is a composite task that combines the two tasks.  A composite task is necessary because VSCode does not allow you to specify multiple tasks in the `preLaunchTask` argument of the `launch.json` file.\n\n```js\n// https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/.vscode/tasks.json\n// this file is used by launch.json\n{\n    \"version\": \"2.0.0\",\n    \"tasks\": [\n      // this task changes into the devtools directory and deletes the temp_debug/axolotl_outputs folder\n      {\n        \"label\": \"delete-outputs\",\n        \"type\": \"shell\",\n        \"command\": \"rm -rf temp_debug/axolotl_outputs\",\n        \"options\":{ \"cwd\": \"${workspaceFolder}/devtools\"},\n        \"problemMatcher\": []\n      },\n      // this task changes into the devtools directory and deletes the `temp_debug/.hf-cache/datasets` folder\n      {\n        \"label\": \"delete-temp-hf-dataset-cache\",\n        \"type\": \"shell\",\n        \"command\": \"rm -rf temp_debug/.hf-cache/datasets\",\n        \"options\":{ \"cwd\": \"${workspaceFolder}/devtools\"},\n        \"problemMatcher\": []\n      },\n        // this task combines the two tasks above\n      {\n       \"label\": \"cleanup-for-dataprep\",\n       \"dependsOn\": [\"delete-outputs\", \"delete-temp-hf-dataset-cache\"],\n      }\n    ]\n}\n```\n\n### Customizing your debugger\n\nYour debugging use case may differ from the example above.  The easiest thing to do is to put your own axolotl config in the `devtools` folder and modify the `launch.json` file to use your config.  You may also want to modify the `preLaunchTask` to delete different folders or not delete anything at all.\n\n### Video Tutorial\n\nThe following video tutorial walks through the above configuration and demonstrates how to debug with VSCode:\n\n{{< video https://youtu.be/xUUB11yeMmc >}}\n\n\n## Debugging With Docker\n\nUsing [official Axolotl Docker images](https://hub.docker.com/r/winglian/axolotl/tags) is a great way to debug your code, and is a very popular way to use Axolotl.  Attaching VSCode to Docker takes a few more steps.  \n\n### Setup\n\nOn the host that is running axolotl (ex: if you are using a remote host), clone the axolotl repo and change your current directory to the root:\n\n```bash\ngit clone https://github.com/OpenAccess-AI-Collective/axolotl\ncd axolotl\n```\n\n:::{.callout-tip}\nIf you already have axolotl cloned on your host, make sure you have the latest changes and change into the root of the project.\n:::\n\nNext, run the desired docker image and mount the current directory. Below is a docker command you can run to do this:[^2]\n\n```bash\ndocker run --privileged --gpus '\"all\"' --shm-size 10g --rm -it --name axolotl --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --mount type=bind,src=\"${PWD}\",target=/workspace/axolotl -v ${HOME}/.cache/huggingface:/root/.cache/huggingface winglian/axolotl:main-py3.10-cu118-2.0.1\n```\n\n:::{.callout-tip}\nTo understand which containers are available, see the [Docker section of the README](../README.md#docker) and the [DockerHub repo](https://hub.docker.com/r/winglian/axolotl/tags).  For details of how the Docker containers are built, see axolotl's [Docker CI builds](../.github/workflows/main.yml).\n:::\n\nYou will now be in the container.  Next, perform an editable install of Axolotl:\n\n```bash\npip3 install packaging\npip3 install -e '.[flash-attn,deepspeed]'\n```\n\n### Attach To Container\n\nNext, if you are using a remote host, [Remote into this host with VSCode](https://code.visualstudio.com/docs/remote/ssh).  If you are using a local host, you can skip this step.\n\nNext, select `Dev Containers: Attach to Running Container...` using the command palette (`CMD + SHIFT + P`) in VSCode.  You will be prompted to select a container to attach to.  Select the container you just created.  You will now be in the container with a working directory that is at the root of the project.  Any changes you make to the code will be reflected both in the container and on the host.\n\nNow you are ready to debug as described above (see [Debugging with VSCode](#debugging-with-vscode)). \n\n### Video - Attaching To Docker On Remote Host\n\nHere is a short video that demonstrates how to attach to a Docker container on a remote host:\n\n{{< video https://youtu.be/0AuoR7QnHR0 >}}\n\n[^1]: The config actually mimics the command `CUDA_VISIBLE_DEVICES=0 python -m accelerate.commands.launch -m axolotl.cli.train devtools/sharegpt.yml`, but this is the same thing.\n\n[^2]: Many of the below flags are recommended best practices by Nvidia when using nvidia-container-toolkit.  You can read more about these flags [here](https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html)."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":["../../../quarto_filter.py"],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","reference-location":"document","css":["../../../styles.css"],"toc":true,"syntax-definitions":["../../../fomo.xml"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","exeucte":{"eval":false},"theme":["solar","../../../custom.scss"],"sidebar":false,"title":"axolotl start here","description":"Best practices for debugging axolotl with an example VSCode config.","categories":["LLMs","fine-tuning","axolotl"],"author":"Hamel Husain","date":"2024-01-11","image":"images/debug_axolotl_small.png","order":1},"extensions":{"book":{"multiFile":true}},"format":{"identifier":{},"render":{},"execute":{},"pandoc":{},"language":{},"metadata":{"title":"axolotl start here","description":"Best practices for debugging axolotl with an example VSCode config.","categories":["LLMs","fine-tuning","axolotl"],"author":"Hamel Husain","date":"2024-01-11","image":"images/debug_axolotl_small.png","order":1}},"active":true}},"projectFormats":["html"]}