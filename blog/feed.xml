<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>kareem&#39;s Blog</title>
<link>https://kareemai.com/blog/feed.html</link>
<atom:link href="https://kareemai.com/blog/feed.xml" rel="self" type="application/rss+xml"/>
<description>Kareem Elkhateb personal site, a blog about machine learning, deep learning, Web developement with Astrojs,fastapi aslo arabic Natural Language Processing and Rust language.</description>
<image>
<url>https://kareemai.com/quarto.png</url>
<title>kareem&#39;s Blog</title>
<link>https://kareemai.com/blog/feed.html</link>
<height>86</height>
<width>144</width>
</image>
<generator>quarto-1.4.549</generator>
<lastBuildDate>Sun, 31 Dec 2023 22:00:00 GMT</lastBuildDate>
<item>
  <title>My little Dragon “kobo”</title>
  <link>https://kareemai.com/blog/posts/mteb_encoding/my_little_dargon.html</link>
  <description><![CDATA[ 




<section id="table-of-contents" class="level2">
<h2 class="anchored" data-anchor-id="table-of-contents">Table of contents</h2>
</section>
<section id="the-time-i-spend-with-kobo" class="level2">
<h2 class="anchored" data-anchor-id="the-time-i-spend-with-kobo">The time I spend with kobo</h2>
<p>I spend on average 8 hours on my laptop “kobo” doing a lot of programming, machine learning experiments and browsing many taps.</p>
<p>I don’t use my phone a lot on average 1 or 2 hours, so i wake up and go to see my little friend and start journaling, wirte my ideas, perparing my todo list.</p>
<p>I have been using it for 4 years now, and it was a nice period i love that it hasn’t broken till now, everything is working good even the battery can stand for the 2.5 hours in the battery life with a lot of memory and cpu usage</p>
<p>You can easily say that my laptop is now a part of me i can’t live without it any more my dairy, movies, work and college is depending on it</p>
</section>
<section id="who-is-kobo" class="level2">
<h2 class="anchored" data-anchor-id="who-is-kobo">Who is “Kobo”!</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kareemai.com/blog/posts/mteb_encoding/dragon_2.png" class="img-fluid figure-img"></p>
<figcaption>Kobo</figcaption>
</figure>
</div>
<section id="system-specifications" class="level3">
<h3 class="anchored" data-anchor-id="system-specifications">System Specifications</h3>
<p>My laptop is a Gfthin 95 core i7 9th gen, 16 GB RAM, 512GB SSD and GTX 1660 Ti, 120Hz screen</p>
<hr>
</section>
</section>
<section id="msi-vs-lenovo" class="level2">
<h2 class="anchored" data-anchor-id="msi-vs-lenovo">MSI vs Lenovo</h2>
<section id="msi-with-linux" class="level3">
<h3 class="anchored" data-anchor-id="msi-with-linux">MSI with Linux</h3>
<p>After buying it, it wasn’t configured with Windows, but was another DOS version so I had to install a Windows version on it. After 1 week I get a lot of errors on it with the external monitor and the WiFi stops working or is not configured. I tried every Hindi video about this problem and gave it to specialists to try it, but in vain, the problem happened again and again.</p>
<p><strong>The solution:</strong> It was my first year at a computer science college, so the geeky thing was to use Linux. After some search, I installed Ubuntu and then tried a lot of distros like Void Linux, but the one that I picked and still use is the amazing PopOS! which is based on Ubuntu but better than it!<br>
The WiFi problem doesn’t appear again! And for Nvidia graphics, it’s solved in PopOS! which has a command to install some drivers. And everything works smoothly</p>
</section>
<section id="the-cost-of-it" class="level3">
<h3 class="anchored" data-anchor-id="the-cost-of-it">The Cost of It!</h3>
<p>I bought it for the cost of 20,000 EGP, which at the time I bought it was $X, but now 20,000 EGP is worth $XX due to problems in my country.</p>
<p>Its cost at that time is really high for my budget and family, but I insisted on buying it because I knew it would stay with me for years, so I wanted a good one.</p>
</section>
<section id="how-i-decided-to-buy-it" class="level3">
<h3 class="anchored" data-anchor-id="how-i-decided-to-buy-it">How I Decided to Buy It!</h3>
<p>The phrase “Machine learning needs an Nvidia GPU” was dancing in my mind, so my main focus was on a nice GPU card in a laptop with a low budget. After some search, I found that the best choices are MSI or Lenovo, but the Lenovo version cost about 4000 EGP more than the MSI.</p>
<blockquote class="blockquote">
<p>I tested the keyboard of the Lenovo, and every time I felt like I wanted to cut my fingers off</p>
</blockquote>
<p>It wasn’t available online, so I had to travel 3 hours to the store that sells it. It was a nice experience to buy something I wanted instead of a random gift from my father.</p>
<hr>
</section>
</section>
<section id="i-am-not-a-gamer" class="level2">
<h2 class="anchored" data-anchor-id="i-am-not-a-gamer">I Am Not a Gamer!</h2>
<p>I didn’t use it for gaming because I am not a gamer, but I used it heavily in programming and some machine learning and basic computer vision models, and it works so nicely!</p>
</section>
<section id="kobo-for-ml" class="level2">
<h2 class="anchored" data-anchor-id="kobo-for-ml">Kobo for ML</h2>
<p><img src="https://kareemai.com/blog/posts/mteb_encoding/dragon_3.png" class="img-fluid" alt="Kobo"> Machine learning is a wide area, and you can use the GPU for 3 things:</p>
<ol type="1">
<li>Train a model from scratch</li>
<li>Fine-tune a model</li>
<li>Evaluate a model (inference)</li>
</ol>
<ul>
<li><strong>Train</strong> a model from scratch was an ideal task for such a GPU, but I was doing it for multiple computer vision classification tasks, learning new architectures, trying to build…etc. For ML you don’t always need a GPU but for deep learning, which is a subset of ML that uses more connected layers (which means more computing power and memory needs), you do.</li>
<li><strong>Fine-tune</strong> a model is the case I used the most - I download a trained model and try to make it work better on my data. What you need here is enough GPU RAM - in my case it’s a 6GB card. This worked fine with computer vision algorithms and classic NLP models with less than 1 billion parameters. There’s no chance to try the LLAMAS models on such a card!</li>
<li><strong>Evaluate</strong> a model means to just load the model and not fine-tune it.</li>
</ul>
<p>I was fine with all the ML tasks I tried to do, unless I opened the door to large language models like the GPT family (ChatGPT for example). These models require a lot of memory and need a good graphics card like an RTX 30 or 40 series to test, and there’s no chance to train these models on any RTX card!</p>
<section id="why-not-just-use-the-cloud" class="level3">
<h3 class="anchored" data-anchor-id="why-not-just-use-the-cloud">Why Not Just Use the Cloud!</h3>
<p>There are two main solutions:</p>
<ol type="1">
<li>Kaggle
<ul>
<li>Kaggle has two GPU options, and I used it a lot, especially if the data was already hosted on Kaggle and was more than 10GB. Other than that, I downloaded a sample of the data and did my work on my own environment with CLI commands and VS Code configurations. This helped me a lot compared to just using the Kaggle editor, and it’s faster too! Sometimes the Kaggle kernel panics or just stops responding entirely!</li>
</ul></li>
<li>Colab
<ul>
<li>Colab restarts after 4 hours and your work can get lost, and lots of annoying things like that happen a lot.</li>
<li>You have to pay for the GPU version. You get a number of hours to try it out, and it’s faster than my GTX 1660 by a good margin.</li>
</ul></li>
</ol>
<p>But I didn’t love these cloud solutions, and I found having my own local setup to be faster and more comfortable.</p>
</section>
</section>
<section id="kobo-for-web-development" class="level2">
<h2 class="anchored" data-anchor-id="kobo-for-web-development">Kobo for Web Development</h2>
<p>I sometimes work on web projects (Django and JavaScript frameworks, especially AstroJS). I never had any issues building and testing web projects - everything worked nicely and efficiently.</p>
</section>
<section id="the-battery-life" class="level2">
<h2 class="anchored" data-anchor-id="the-battery-life">The Battery Life</h2>
<p>The battery life can keep it working for 2.5 hours when the power is out if you switch to battery saver mode. I don’t think it could last more than 1 hour in normal mode! This is the laptop’s biggest issue. Sometimes it powers off even when fully charged if you do a lot of computation without plugging it in.</p>
</section>
<section id="the-heat" class="level2">
<h2 class="anchored" data-anchor-id="the-heat">The Heat</h2>
<p><img src="https://kareemai.com/blog/posts/mteb_encoding/dragon_4.png" class="img-fluid" alt="target"> I didn’t find heat to be a problem or even the fan noise in most of my work. But sometimes when running a deep learning model, the fan noise is a bit loud and it does get really hot. For normal coding and browsing though, there are no issues and the cooling system is fast. Overall I didn’t find heat to be a major problem.</p>
</section>
<section id="the-keyboard" class="level2">
<h2 class="anchored" data-anchor-id="the-keyboard">The Keyboard</h2>
<p>The keyboard is very responsive and well configured with smooth clicks - no issues or sticky keys even though I have fat fingers!</p>
<p>I’ve been using this machine for 4 years and I write a lot. No broken keys have happened despite my clumsy fingers. I have a mechanical keyboard that I sometimes use, but I always miss the feel of the built-in keyboard. The red backlight is decent.</p>
</section>
<section id="finally" class="level2">
<h2 class="anchored" data-anchor-id="finally">Finally!</h2>
<p><img src="https://kareemai.com/blog/posts/mteb_encoding/draong_1.png" class="img-fluid" alt="Kobo_finally"> I just wanted to say that I love “Kobo” and I’ve spent nice times with it - some really difficult and others really happy. It’s been a loyal friend and something to rely on.</p>


</section>

 ]]></description>
  <guid>https://kareemai.com/blog/posts/mteb_encoding/my_little_dargon.html</guid>
  <pubDate>Sun, 31 Dec 2023 22:00:00 GMT</pubDate>
</item>
<item>
  <title>After one year of using Huawei Mate 11 without google services</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/blog/posts/products_reviews/Huawei_mate_pad_11.html</link>
  <description><![CDATA[ 




<section id="table-of-contents" class="level2">
<h2 class="anchored" data-anchor-id="table-of-contents">Table of contents</h2>
<p>Here is an expanded version of your blog post with some additional details:</p>
</section>
<section id="why-i-bought-it" class="level2">
<h2 class="anchored" data-anchor-id="why-i-bought-it">Why I bought it</h2>
<p>I purchased the Huawei tablet because I enjoy creative activities like drawing, taking notes, and reading books. I wished to have an iPad or premium tablet with a good stylus to fully explore my creative potential. My brother found an excellent deal on this Huawei tablet for around 60% off retail price and gifted it to me. I’m very thankful to him! <img src="https://consumer.huawei.com/content/dam/huawei-cbg-site/common/mkt/pdp/tablets/matepad-11-2023-new/images/kv/Huawei-matepad-11-inch-2023-kv@2x.webp" class="img-fluid"></p>
</section>
<section id="pros" class="level2">
<h2 class="anchored" data-anchor-id="pros">Pros</h2>
<ol type="1">
<li>Excellent audio quality with quad speakers</li>
<li>Gorgeous 10.95-inch LCD display
<ul>
<li>2560x1600 resolution</li>
<li>120Hz refresh rate for smooth visuals</li>
</ul></li>
<li>Fast charging capabilities
<ul>
<li>Large 7250 mAh battery</li>
<li>Supports 22.5W fast wired charging</li>
</ul></li>
<li>Comfortable lightweight design, easy to use for long periods</li>
<li>Seamless integration with other Huawei devices like watches and earbuds</li>
<li>Responsive stylus with good palm rejection</li>
</ol>
</section>
<section id="cons" class="level2">
<h2 class="anchored" data-anchor-id="cons">Cons</h2>
<ol type="1">
<li>Tablet gets hot with prolonged intensive use like drawing or taking notes. Quite annoying.</li>
<li>Battery life is decent but not enough to last a full day with heavy usage</li>
<li>Frequent ads in App Gallery and default music app are frustrating
<ul>
<li>Should not see ads just trying to open App Gallery</li>
<li>Default music app tries to push online streaming service with more ads</li>
</ul></li>
<li>Many apps spam notifications asking to reopen them. Very irritating.</li>
</ol>
</section>
<section id="using-huawei-devices-with-linux" class="level2">
<h2 class="anchored" data-anchor-id="using-huawei-devices-with-linux">Using Huawei Devices with Linux</h2>
<ul>
<li>File transfers require a cable, no wireless sharing</li>
<li>Must use third party apps like KDE Connect for notifications</li>
<li>Can’t develop custom themes or scripts due to lack of developer tools</li>
<li>Syncing to Linux with Syncthing is slow and buggy</li>
<li>Overall poor integration with Linux compared to Android/Windows</li>
</ul>
</section>
<section id="issues-with-google-services-on-huawei" class="level2">
<h2 class="anchored" data-anchor-id="issues-with-google-services-on-huawei">Issues with Google Services on Huawei</h2>
<section id="stylus-apps" class="level3">
<h3 class="anchored" data-anchor-id="stylus-apps">Stylus Apps</h3>
<ul>
<li><strong>Infinite Painter</strong> drawing app has broken stylus support and no pressure sensitivity</li>
<li><strong>NoteShelf</strong> note taking app can’t sync properly with cloud drives</li>
<li><strong>Nebo</strong> note taking app has subpar palm rejection and no Arabic language support</li>
<li><strong>Flexcil</strong> best ebook reader/annotator but lacks Google Drive integration</li>
</ul>
</section>
<section id="gbox-solution" class="level3">
<h3 class="anchored" data-anchor-id="gbox-solution">Gbox Solution</h3>
<ul>
<li>Provides access to YouTube, Maps and other Google apps</li>
<li>Available on App Gallery with native integration</li>
<li>Minimal battery drain</li>
<li>But lacks support for many Google services like Podcasts</li>
<li>Buggy syncing with Google Keep</li>
<li>Can’t leverage Google Drive within other apps</li>
</ul>
</section>
<section id="apk-pure" class="level3">
<h3 class="anchored" data-anchor-id="apk-pure">APK Pure</h3>
<ul>
<li>Apps sometimes fail to open, just black screen</li>
<li>Too many ads</li>
<li>Lacks reliability compared to Play Store</li>
</ul>
<p>Overall, while the Huawei tablet offers excellent hardware, the software experience is hampered by lack of Google services. This leads to janky third party solutions and poor integration with Linux systems. I still enjoy using the tablet but hope someday Huawei can properly resolve these issues.</p>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">references</h2>
<ul>
<li>https://www.noteshelf.net/</li>
<li>https://www.gboxlab.com/</li>
<li>https://www.infinitestudio.art/painter.php</li>
</ul>


</section>

 ]]></description>
  <guid>https://kareemai.com/blog/posts/products_reviews/Huawei_mate_pad_11.html</guid>
  <pubDate>Wed, 06 Dec 2023 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Huawei Freebuds 5i review: is it the best budget earbuds for Huawei phone owner!</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/blog/posts/products_reviews/Huawei freebuds 5i.html</link>
  <description><![CDATA[ 




<section id="table-of-contents" class="level2">
<h2 class="anchored" data-anchor-id="table-of-contents">Table of contents</h2>
</section>
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>Hi, In this fast blog i will talk about my review with Huawei Freebuds 5i, which is the first noise-cancelling earbuds i have tried. This is not an ad or even affiliate product, it’s my own review for fun and just seeking of knowledge. <img src="https://consumer.huawei.com/content/dam/huawei-cbg-site/common/mkt/pdp/headphones/freebuds5i/imges/huawei-freebuds-5i-kv.jpg" class="img-fluid" alt="freebuds"></p>
</section>
<section id="who-am-i" class="level2">
<h2 class="anchored" data-anchor-id="who-am-i">Who am i !</h2>
<ul>
<li>I am Kareem, a college student interested in machine learning and web technology.</li>
<li>I enjoy working deeply and seeking out quiet places, but I live in an area with some noisy disturbances in the mornings - people walking down the street, kids playing around the house, street vendors - which often interrupt my focus and break my state of flow.</li>
<li>I wish there was a solution to block out all these sounds so I could live in an isolated place free of annoying noises! Using noise-cancelling headphones or heading into outer space would be ideal environments with no bothersome sounds around.</li>
</ul>
</section>
<section id="how-i-bought-it" class="level2">
<h2 class="anchored" data-anchor-id="how-i-bought-it">how i bought it</h2>
<ul>
<li>I bought it from amazon prime, and it cost me around 100$ or 3,200 pounds. It reached within two days.</li>
</ul>
</section>
<section id="first-impressions" class="level2">
<h2 class="anchored" data-anchor-id="first-impressions">First impressions</h2>
<ul>
<li>Upon unboxing and using it for the first time, I was genuinely pleased with the initial track I played.</li>
<li>The sound quality was noticeably superior compared to my phone or any other earbuds I’ve previously used. I then tested the noise-cancelling feature and was amazed when I couldn’t hear my younger brother calling me.</li>
<li>The street noises were effectively blocked out, with only a faint hum reaching my ears, which was quite tolerable. Initially, the earbud tips were a bit uncomfortable, but after switching to a smaller size, the discomfort was resolved. Everything seemed perfect, except for a slight pressure in my ears when using the noise-cancelling feature, particularly noticeable the following morning. Despite this, my ears seem to crave the earbuds and my skin seems to miss them when they’re not in use. This can be a bit of a downside, but it’s not always the case.</li>
</ul>
</section>
<section id="is-noise-cancelling-actually-work" class="level2">
<h2 class="anchored" data-anchor-id="is-noise-cancelling-actually-work">Is noise cancelling actually work ?</h2>
<p>I’ve used these earbuds in various settings:</p>
<ol type="1">
<li><p>Subway: The noise-cancelling feature combined with my music created an immersive experience, effectively blocking out any other sounds and allowing me to enjoy my tunes in peace.</p></li>
<li><p>College: Amidst the chatter and shouts of other students, I was able to retreat into my own world of sound.</p></li>
<li><p>Bus: The noise of the bus engine was successfully blocked out, but I could still hear the conversations of those sitting next to me. It wasn’t overly bothersome, but it didn’t provide complete isolation.</p></li>
</ol>
<p>In summary, the noise-cancelling feature works well, but it doesn’t provide absolute silence. I can still hear some ambient sounds. One noticeable issue is that when I make calls using the earbuds, I can hear the other person clearly, but they often complain about the clarity of my voice. They say it sounds distant and unclear, sometimes even asking to call back later. This issue is particularly prevalent when I’m on the bus.</p>
</section>
<section id="the-modes-of-the-freebuds-and" class="level2">
<h2 class="anchored" data-anchor-id="the-modes-of-the-freebuds-and">the modes of the Freebuds and</h2>
<p>Here is a rephrased version of the key points about the Huawei Freebuds 5i’s features:</p>
<p><strong>Noise Cancellation Modes:</strong></p>
<ol type="1">
<li><p><strong>Noise Cancelling Mode</strong>: Actively cancels out ambient noise. Uses more battery power.</p></li>
<li><p><strong>Off Mode</strong>: No active noise cancellation, but still provides some passive noise isolation. Less battery usage.</p></li>
<li><p><strong>Awareness Mode</strong>: Allows ambient sounds to be heard clearly. Useful for hearing announcements at the gym or people talking to you. However, my own voice sounds muted in this mode, so I have to remove the earbuds temporarily to hold conversations.</p></li>
</ol>
<p><strong>Sound Quality Presets:</strong></p>
<ol type="1">
<li><p><em>Default</em>: No sound enhancements applied.</p></li>
<li><p><em>Treble Boost</em>: Boosts treble frequencies. I haven’t used this.</p></li>
<li><p><em>Bass Boost</em>: Emphasizes bass when listening to music like hip-hop.</p></li>
<li><p><em>Voices</em>: Optimizes sound for speech clarity. I use this for podcasts.</p></li>
</ol>
<p><strong>Connection Priority Modes:</strong></p>
<ol type="1">
<li><p><strong>Balance Mode</strong>: Balances audio quality and connection stability.</p></li>
<li><p><strong>Sound Quality Priority</strong>: Prioritizes sound quality over connectivity. Uses more power which may cause occasional lag.</p></li>
</ol>
<p>I have not noticed any discernible difference between these two modes, so I just use the Balance mode.</p>
</section>
<section id="the-gestures" class="level2">
<h2 class="anchored" data-anchor-id="the-gestures">The Gestures</h2>
<p>The Huawei Freebuds 5i offer various gesture controls:</p>
<ul>
<li>Double-tap, Triple-tap, Press &amp; hold, Swipe</li>
</ul>
<p>I particularly enjoy the swipe gesture for volume control, as it works smoothly. The press and hold gesture is a bit slow, and the triple-tap gesture can be annoying since tapping your ear three times isn’t very comfortable.</p>
<p>Occasionally, the Freebuds don’t recognize when I’ve inserted the left or right earbud, and I continue listening with just one earbud without realizing the other isn’t connected. This doesn’t happen often, though.</p>
</section>
<section id="the-compatibility-with-other-devices" class="level2">
<h2 class="anchored" data-anchor-id="the-compatibility-with-other-devices">The compatibility with other devices</h2>
<ul>
<li>I love how it connect with both the phone and tablet without any problems or conflict</li>
<li>I’ve been using the Freebuds with my Realme phone, MSI Linux laptop, and Huawei Mate Pad 11.</li>
<li>The connection with the Realme phone is seamless, regardless of whether the AI Life application is used or not.</li>
<li>When it comes to the Huawei Mate Pad 11, there’s a feature that allows you to adjust settings directly from the Bluetooth menu. This is a functionality that isn’t available on standard Android devices without the AI Life app.</li>
</ul>
<section id="linux-connection" class="level3">
<h3 class="anchored" data-anchor-id="linux-connection">Linux connection</h3>
<p>I’ve connected the Freebuds to my Linux laptop using the ‘<a href="https://smarttech101.com/bluetoothctl-management-of-bluetooth-devices-in-linux/">bluetoothctl</a>’ command. However, I’ve encountered an issue where I need to restart Bluetooth each time I want to establish a connection for it to work properly</p>
</section>
</section>
<section id="the-case-and-overall-design" class="level2">
<h2 class="anchored" data-anchor-id="the-case-and-overall-design">The case and overall design</h2>
<ul>
<li>The design of the Freebuds is truly appealing. It has a unique aesthetic that sets it apart from other earbuds, giving it a premium feel.</li>
<li>In terms of design, the FreeBuds 5i bear a resemblance to the 4i model, but they are 11% lighter and have shorter stems. They come with an IP54 rating, making them dust-tight and splash-resistant. The color options include a new “Isle Blue”, along with “Nebula Black” and “Ceramic White”. The package includes small, medium, and large silicone eartips, as well as a short USB-C cable for charging the case.</li>
<li>I love the sound of closing it, it’s a loud sound but lovely :)</li>
</ul>
</section>
<section id="the-battery" class="level2">
<h2 class="anchored" data-anchor-id="the-battery">The battery</h2>
<ul>
<li>The battery is really nice; I haven’t felt that I am missing the need to recharge it or that it has gone off at any time.</li>
</ul>
<section id="battery-capacity" class="level3">
<h3 class="anchored" data-anchor-id="battery-capacity">Battery capacity</h3>
<ul>
<li>Per earbud: 55 mAh (min)*</li>
<li>Charging case: 410 mAh (min)*</li>
</ul>
</section>
<section id="playtime" class="level3">
<h3 class="anchored" data-anchor-id="playtime">Playtime</h3>
<ul>
<li>Music playback on 1 charge: 6.0 hours (with ANC enabled)**</li>
<li>Music playback on 1 charge: 7.5 hours (with ANC disabled)**</li>
<li>Music playback with charging case: 18.5 hours (with ANC enabled)**</li>
<li>Music playback with charging case: 28 hours (with ANC disabled)**</li>
</ul>
</section>
<section id="charging-time" class="level3">
<h3 class="anchored" data-anchor-id="charging-time">Charging Time</h3>
<ul>
<li>About 60 minutes for the earbuds (in the charging case)***</li>
<li>About 110 for charging case without earbuds (wired)***</li>
</ul>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>https://smarttech101.com/bluetoothctl-management-of-bluetooth-devices-in-linux/</li>
<li>https://askubuntu.com/questions/1225896/huawei-freebuds-3-pairing-with-ubuntu-18-04</li>
</ul>


</section>

 ]]></description>
  <guid>https://kareemai.com/blog/posts/products_reviews/Huawei freebuds 5i.html</guid>
  <pubDate>Tue, 05 Dec 2023 22:00:00 GMT</pubDate>
</item>
<item>
  <title>MTEB Massive Text Embedding Benchmark</title>
  <link>https://kareemai.com/blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html</link>
  <description><![CDATA[ 




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>#defintion MTEB: MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB.</p>
</section>
<section id="embedding-models" class="level2">
<h2 class="anchored" data-anchor-id="embedding-models">Embedding models</h2>
<ul>
<li>Text embedding models like GLove lack context awareness and ar=e thus commonly labeled as word embedding model. They consist of a layer mapping each input word to a vector often followed by an averaging layer to provide a final embedding invariant of input length.</li>
<li>Transformers inject context awareness into language models via self-attention and form the foundation of most recent embedding models.
<ul>
<li>BERT uses the transformer architecture and performs large-scale self-supervised pre-training. The resulting model can directly be used to produce text embeddings via an averaging operation alike Glove.</li>
<li>SBERT be beneficial to perform additional fine-tuning of the transformer for competitive embedding performance.</li>
<li><em>Most recent fine-tuned embedding models use a contrastive loss objective to perform supervised fine-tuned on positive and negative text pairs</em></li>
<li>#critique Due to the large variety of available pretrained transformers,there is an at least equally large variety of potential text embedding models to be explored.This leads to confusion about which model provides practitioners with the best performance for their embedding use case.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="the-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-problem">The problem</h2>
<ul>
<li>The problem with the current evaluation regime of current text embedding models rarely covers the breadth of their possible use cases.
<ul>
<li>#example SimCSE or SBERT solely evaluate on STS and classification tasks,leaving open questions about the transfer ability of the embedding models to search or clustering tasks.</li>
</ul></li>
<li>evaluating embedding methods on many tasks requires implementing multiple evaluation pipelines.</li>
<li>implementation details like preprocessing or hyperparameters may influence the results making it unclear whether performance improvements simply come from a favorable evaluation pipeline. This leads to the “blind” application of these models to new use cases in industry or requires incremental work to reevaluate them on different tasks.</li>
</ul>
<hr>
</section>
<section id="the-solution-with-this-benchmark" class="level2">
<h2 class="anchored" data-anchor-id="the-solution-with-this-benchmark">The solution with this benchmark</h2>
<ul>
<li><strong>MTEB</strong> consists of 58 datasets covering 112 languages from 8 embedding tasks:</li>
</ul>
<ol type="1">
<li>Bitext mining</li>
<li>classification</li>
<li>clustering</li>
<li>pair classification</li>
<li>reranking, retrieval</li>
<li>STS</li>
<li>summarization.</li>
</ol>
<ul>
<li>MTEB software is available open-source1 enabling evaluation of any embedding model by adding less than 10 lines of code.</li>
<li>Datasets and the MTEB leaderboard are available on the Hugging Face Hub2 .</li>
<li>We evaluate over 30 models on MTEB with additional speed and memory benchmarking to provide a holistic view of the state of text embedding models. We cover both models available open-source as well as models accessible via APIs, such as the OpenAI Embeddings endpoint</li>
<li>It aims to sheds light on the weaknesses and strenghts of individual models,such as SimCSE’s<a href="https://doi.org/10.48550/ARXIV.2004.07180">(Gao et al., 2021b)</a> low performance on clustering and retrieval despite its strong performance on STS.</li>
</ul>
</section>
<section id="the-mteb-desiderata" class="level2">
<h2 class="anchored" data-anchor-id="the-mteb-desiderata">The MTEB Desiderata</h2>
<p>METB is build on a set of desiderat.</p>
<ol type="1">
<li><strong>Diversity</strong>:
<ul>
<li>it consists of 58 total datasets, 10 are multilingual, covering 112 different langauges.</li>
<li>Sentence-level and paragraph level datasets are included to contrast performance on short and long texts.</li>
</ul></li>
<li><strong>Simplicity</strong>
<ul>
<li>It provides a simple API for plugging in any model that given a list of text can produce a vector for each list of texts can produce a vector for each list item with a consistent shape.</li>
</ul></li>
<li><strong>Extensibility</strong>
<ul>
<li>you can add new datasets for existing tasks via a single file that specifies the task and a Huggingface dataset name where the data has been uploaded.</li>
<li>New tasks require implementing a task interface for loading the data and an evaluator for benchmarking</li>
</ul></li>
<li><strong>Reproduciblity</strong>
<ul>
<li>Through versioning at a dataset and software level,they make it easy to reproduce results in METP.</li>
<li>JSON files corresponding to all results available in this paper have been made available together with the MTEB benchmark</li>
</ul></li>
</ol>
</section>
<section id="tasks-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="tasks-and-evaluation">Tasks and Evaluation</h2>
<p>#definition <strong>Bitext Mining</strong><br>
Inputs are two sets of sentences from two different languages. For each sentence in the first set, the best match in the second set needs to be found.</p>
<ul>
<li>The matches are commonly translations.</li>
<li>The Provided model i used to embed each sentence and the closest pairs are found via cosine similarity</li>
<li>F1 serves as the main metric for bitext mining.Accuracy, precision and recall are also computed. #definition <strong>Classification</strong></li>
<li>A train and test set are embedded with the provided model.</li>
<li>The train set embeddings are used to train a logistic regression classifier with 100 maximum iterations, which is scored on the test set.</li>
<li>The main metric is accuracy with average precision and f1 additionally provided. #definition <strong>Clustering</strong> Given a set of sentences or paragraphs, the goal is to group them into meaningful clusters.</li>
<li>A mini-batch k-means model with batch size 32 and k equal to the number of different labels is trained on the embedded texts.</li>
<li>The model scored using V-measure - V-measure does not depend on the cluster label,thus the permutation of labels does not affect the score. #definition <strong>Classification</strong> A pair of text inputs is provided and a label needs to be assigned. Labels are typically binary variables denoting duplicate or paraphrase pairs.</li>
<li>The two texts are embedded and their distance is computed with various metrics (cosine similarity, dot product, euclidean distance, manhattan distance).</li>
<li>Using the best binary thresh- old accuracy, average precision, f1, precision and recall are computed.</li>
<li>The average precision score based on cosine similarity is the main metric. #definition <strong>Reranking</strong> : Inputs are a query and a list of relevant and irrelevant reference texts. The aim is to rank the results according to their relevance to the query.</li>
<li>The model is used to embed the references which are then compared to the query using cosine similarity.</li>
<li>The resulting ranking is scored for each query and averaged across all queries.</li>
<li>Metrics are mean MRR@k and MAP with the latter being the main metric. #definition <strong>Retrieval</strong> Each dataset consists of a corpus, queries and a mapping for each query to relevant documents from the corpus.</li>
<li>The aim is to find these relevant documents.</li>
<li>The provided model is used to embed all queries and all corpus documents and similarity scores are computed using cosine similarity. After ranking the corpus documents for each query based on the scores, nDCG@k, MRR@k, MAP@k, precision@k and recall@k are computed for several values of k. nDCG@10 serves as the main metric.</li>
<li>MTEB reuses datasets and evaluation from BEIR (Thakur et al., 2021). #definition <strong>Semantic Textual Similarity (STS)</strong> Given a sentence pair the aim is to determine their similarity. Labels are continuous scores with higher numbers indicating more similar sentences.</li>
<li>The provided model is used to embed the sentences and their similarity is computed using various distance metrics.</li>
<li>Distances are benchmarked with ground truth similarities using Pearson and Spearman correlations.</li>
<li>Spearman correlation based on cosine similarity serves as the main metric (Reimers et al.,2016). #definition <strong>Summarization</strong> A set of human-written and machine-generated summaries are provided. The aim is to score the machine summaries. The provided model is first used to embed all summaries. For each machine summary embedding, distances to all human summary embeddings are computed. The closest score (e.g.&nbsp;highest cosine similarity) is kept and used as the model’s score of a single machine-generated summary. Pearson and Spearman correlations with ground truth human assessments of the machine-generated summaries are computed. Like for STS, Spearman correlation based on cosine similarity serves as the main metric</li>
</ul>
<hr>
<p>#sidenote <strong>Non-Transformers</strong> : LASER (Heffernan et al.,2022) is the only context aware non-transformer model we benchmark, relying on an LSTM (Hochreiter and Schmidhuber, 1997) instead. Similar to LaBSE, the model trains on parallel data and focuses on bitext mining applications. ![[Pasted image 20231028124555.png]]</p>
</section>
<section id="analysis" class="level2">
<h2 class="anchored" data-anchor-id="analysis">Analysis</h2>
<ul>
<li>we observe that there is considerable variability between tasks. No model claims the state-of-the-art in all seven English tasks.</li>
<li>There is even more variability in the results per dataset present in the appendix.</li>
<li>Further, there remains a large gap between self-supervised and supervised methods.</li>
<li>Self-supervised large language models have been able to close this gap in many natural language generation tasks (Chowd- hery et al., 2022).</li>
<li>However, they appear to still require supervised fine-tuning for competitive em- bedding performance.</li>
<li>We find that performance strongly coorelates with model size, A majority of MTEB tasks are domainted by multi-billion parameter models.However, these come at a significant cost</li>
<li><strong>For classification</strong>
<ul>
<li>ST5 models dominate the classification task across most datasets</li>
<li>ST5-XXL has the highest average performance, 3% ahead of the best non-ST5 model, OpenAI Ada Similarity</li>
</ul></li>
<li><strong>Clustering</strong>
<ul>
<li>Despite being almost 50x smaller, the MPNet embedding model is on par with the ST5- XXL state-of-the-art on Clustering. This may be due to the large variety of datasets MPNet (and MiniLM) has been fine-tuned on.</li>
<li>Clustering requires coherent distances between a large number of embeddings.</li>
<li>Models like SimCSE-sup or SGPTnli, which are only fine-tuned on a single dataset,NLI, may produce incoherent embeddings when encountering topics unseen during fine-tuning.</li>
<li>Relatedly, we find that the query embeddings of SGPT-msmarco and the Ada Search endpoint are competitive with SGPT-nli and the Ada Similarity endpoint,respectively.</li>
<li>We refer to the public leaderboard5 for Ada Search results. This could be due to the MSMARCO dataset being significantly larger than NLI.</li>
<li>Thus, while the OpenAI docs recommend using the similarity embeddings for clustering use cases6 , the retrieval query embeddings may be the better choice in some cases.</li>
</ul></li>
<li><strong>Pair Classification</strong>
<ul>
<li>GTR-XL and GTR-XXL have the strongest performance. Pair classification is closest to STS in its framing, yet models rank significantly differently on the two tasks. This highlights the importance of benchmarking on a diverse set of tasks to avoid blindly reusing a model for a different task.</li>
</ul></li>
<li><strong>Reranking</strong>
<ul>
<li>MPNet and MiniLM models perform strongly on reranking tasks.</li>
<li>On SciDocsRR (Co-han et al., 2020a) they perform far better than big- ger models, which is likely due to parts of SciDocsRR being included in their training data.</li>
<li>Our scale of experiments and that of model pre-training make controlling for data contamination challenging.</li>
<li>Thus, we ignore overlap of MTEB datasets with model training datasets in MTEB scores.</li>
<li>As long as enough datasets are averaged, we believe these effects to be insignificant.</li>
</ul></li>
<li><strong>Retrieval</strong>
<ul>
<li>SGPT-5.8B-msmarco is the best em- bedding model on the BEIR subset in MTEB as well as on the full BEIR benchmark (Thakur et al., 2021; Muennighoff, 2022).</li>
<li>The even larger 7.1B SGPT model making use of BLOOM (Scao et al., 2022) performs significantly weaker, which is likely due to the multilinguality of BLOOM.</li>
<li>Models geared towards STS (SimCSE, ST5, SGPT- nli) perform badly on retrieval tasks.</li>
<li>Retrieval tasks are unique in that there are two distinct types of texts: Queries and documents (“asymmetric”), while other tasks only have a single type of text (“symmetric”).</li>
<li>On the QuoraRetrieval dataset, which has been shown to be largely symmetric (Muennighoff, 2022), the playing field is more even with SGPT-5.8B-nli outperforming SGPT- 5.8B-msmarco,</li>
</ul></li>
<li><strong>STS &amp; Summarization</strong>
<ul>
<li>Retrieval models (GTR, SGPT-msmarco) perform badly on STS, while ST5-XXL has the highest performance.</li>
<li>This highlights the bifurcation of the field into separate embedding models for retrieval (asymmetric) and similarity (symmetric) use cases (Muennighoff, 2022).</li>
</ul></li>
</ul>
</section>
<section id="efficiency" class="level2">
<h2 class="anchored" data-anchor-id="efficiency">Efficiency</h2>
<p><strong>Maximum speed</strong> -&gt; Word Embedding models offer maximum speed with Glove taking the lead on both performance and speed, thus making the choice simple in this case</p>
<p><strong>Maximum performance</strong> -&gt; If latency is less important than performance Depending on the task at hand, GTR-XXL, ST5-XXL or SGPT-5.8B may be the right choice,</p>
<p><strong>Speed and Peformance</strong> -&gt; The fine-tuned MPNet and MiniLM models lead the middle cluster making the choice easy.</p>
<hr>
<p>#todo you can check the gte architecture here it’s highly related to the MTEP [[tiny-gte_transformer_model]]</p>
<hr>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<ul>
<li>We found model performance on different tasks to vary strongly with no model claiming state-of-the-art on all tasks.</li>
<li>Our studies on scaling behavior, model efficiency and multilinguality revealed various intricacies of models that should ease the decision-making process for future research or industry applications of text embeddings.</li>
</ul>
<hr>
<p>Thanks for reading. If you have any questions, feel free to comment down below or reach out to me on twitter <a href="https://twitter.com/AbdelkareemElk1"><span class="citation" data-cites="AbdelkareemElk1">@AbdelkareemElk1</span></a>.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>https://huggingface.co/blog/mteb</li>
<li>https://arxiv.org/abs/2210.07316</li>
</ul>


</section>

 ]]></description>
  <guid>https://kareemai.com/blog/posts/mteb_encoding/MTEB_massive_text_embedding_benchmark.html</guid>
  <pubDate>Fri, 27 Oct 2023 22:00:00 GMT</pubDate>
</item>
<item>
  <title>tiny-gte “Tiny, yet powerful, it is small in size but packs a lot of power.”</title>
  <link>https://kareemai.com/blog/posts/mteb_encoding/tiny-gte_transformer_model.html</link>
  <description><![CDATA[ 




<section id="table-of-contents" class="level2">
<h2 class="anchored" data-anchor-id="table-of-contents">Table of contents</h2>
<p>#definition : This is a&nbsp;<a href="https://www.sbert.net/">sentence-transformers</a>&nbsp;model: It maps sentences &amp; paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search. It is distilled from&nbsp;<code>thenlper/gte-small</code>, with comparable (slightly worse) performance at around half the size.</p>
</section>
<section id="details" class="level2">
<h2 class="anchored" data-anchor-id="details">Details</h2>
<ul>
<li>It’s around ~45MB very small compared to other models like MiniLM-L6-V2 which is equal to ~80 MB</li>
<li>Embedding vector size 384d</li>
<li>BERT based</li>
<li>Distilied from thenlper/gte-small,</li>
</ul>
</section>
<section id="notice-about-using-small-size" class="level2">
<h2 class="anchored" data-anchor-id="notice-about-using-small-size">Notice about using small size</h2>
</section>
<section id="use-cases" class="level2">
<h2 class="anchored" data-anchor-id="use-cases">Use Cases</h2>
</section>
<section id="todo" class="level1">
<h1>Todo</h1>
<ol type="1">
<li><label><input type="checkbox" checked="">Explain the MTEB</label>
<ol type="1">
<li>Add the link of it into the this blog</li>
</ol></li>
<li>Explain fastembed</li>
<li>Make comparison between tiny-gte and small gte</li>
<li>Add the small-gte-4096 comparison</li>
</ol>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li>https://huggingface.co/TaylorAI/gte-tiny</li>
<li>https://www.linkedin.com/posts/prithivirajdamodaran_%3F%3F%3F%3F-%3F%3F%3F%3F-%3F%3F%3F%3F%3F-%3F%3F%3F%3F-activity-7120279840569597952-iwc-/?utm_source=share&amp;utm_medium=member_desktop</li>
</ul>


</section>

 ]]></description>
  <guid>https://kareemai.com/blog/posts/mteb_encoding/tiny-gte_transformer_model.html</guid>
  <pubDate>Fri, 20 Oct 2023 21:00:00 GMT</pubDate>
</item>
<item>
  <title>How I am using NLP to improve my Websites SEO</title>
  <dc:creator>Kareem Elkhateb</dc:creator>
  <link>https://kareemai.com/blog/posts/seo/how_i_use_nlp_for_seo.html</link>
  <description><![CDATA[ 




<p><strong>Unlocking SEO Potential with NLP: A Journey of Website Optimization</strong></p>
<p>In the vast digital landscape, the visibility of a website amidst the sea of search results can make or break its success. Search Engine Optimization (SEO) stands as the beacon guiding businesses towards the shores of higher visibility and traffic. As the digital realm evolves, so do the strategies employed to enhance SEO. One such frontier that holds immense promise is the integration of Natural Language Processing (NLP) into SEO practices.</p>
<section id="understanding-the-landscape" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-landscape">Understanding the Landscape</h3>
<p>SEO isn’t just about optimizing keywords anymore; it’s about understanding user intent, predicting trends, and delivering value-rich content. Harnessing the power of NLP, we embark on a journey to revolutionize SEO practices for our websites. Here’s how we’re leveraging NLP to bolster our SEO efforts:</p>
</section>
<section id="daily-downloads-and-keyword-integration" class="level3">
<h3 class="anchored" data-anchor-id="daily-downloads-and-keyword-integration">1. Daily Downloads and Keyword Integration</h3>
<p>We kickstart our journey by integrating Google Search Console into our workflow. By fetching daily downloads of our top queries, we gain invaluable insights into user behavior and preferences. These queries are seamlessly integrated into a specific folder, forming the backbone of our content optimization strategy.</p>
</section>
<section id="content-analysis-and-optimization" class="level3">
<h3 class="anchored" data-anchor-id="content-analysis-and-optimization">2. Content Analysis and Optimization</h3>
<p>NLP comes into play as we meticulously analyze our content to ensure alignment with the top queries identified. By leveraging NLP techniques, we ascertain the presence of these keywords within our content, ensuring relevance and resonance with our audience’s search intent.</p>
</section>
<section id="predictive-analytics-and-trend-spotting" class="level3">
<h3 class="anchored" data-anchor-id="predictive-analytics-and-trend-spotting">3. Predictive Analytics and Trend Spotting</h3>
<p>Powered by NLP, we delve into the realm of predictive analytics to forecast future trends. By analyzing historical data and current patterns, we identify emerging trends and capitalize on them proactively. This enables us to stay ahead of the curve and position our content strategically to meet evolving user demands.</p>
</section>
<section id="semantic-and-exact-match-integration" class="level3">
<h3 class="anchored" data-anchor-id="semantic-and-exact-match-integration">4. Semantic and Exact Match Integration</h3>
<p>NLP empowers us to delve deeper into the semantics of language, enabling us to differentiate between exact matches and similar meanings. Through advanced NLP algorithms, we ensure that our content not only features exact keyword matches but also resonates with synonymous terms, enhancing its visibility across diverse search queries.</p>
</section>
<section id="leveling-up-the-game" class="level3">
<h3 class="anchored" data-anchor-id="leveling-up-the-game">Leveling Up the Game</h3>
<p>As we progress on our journey of NLP-driven SEO optimization, we outline a roadmap for leveling up our capabilities:</p>
<section id="level-1" class="level4">
<h4 class="anchored" data-anchor-id="level-1">Level 1:</h4>
<ul>
<li><strong>Data Management</strong>: Implementing robust data storage solutions tailored to multiple websites.</li>
<li><strong>Visualization</strong>: Crafting compelling visualizations using tools like Bokeh and Plotly.</li>
<li><strong>Machine Learning Models</strong>: Developing forecasting models to predict future trends.</li>
<li><strong>Semantic Search</strong>: Leveraging vector databases for semantic and exact search capabilities.</li>
</ul>
</section>
<section id="level-2" class="level4">
<h4 class="anchored" data-anchor-id="level-2">Level 2:</h4>
<ul>
<li><strong>Optimization</strong>: Streamlining our codebase through asynchronous processing and caching mechanisms.</li>
</ul>
</section>
<section id="level-3" class="level4">
<h4 class="anchored" data-anchor-id="level-3">Level 3:</h4>
<ul>
<li><strong>Product Development</strong>: Exploring avenues to transform our NLP-driven SEO framework into software products like WordPress plugins or desktop applications.</li>
</ul>
</section>
<section id="level-4" class="level4">
<h4 class="anchored" data-anchor-id="level-4">Level 4:</h4>
<ul>
<li><strong>Innovation</strong>: Drawing inspiration from industry leaders to create bespoke solutions, such as an Arabic version of our SEO platform akin to UberSuggest and Guni Rank.</li>
</ul>
</section>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>I mainly using NLP with python to improve my landing pages for home services like the following services: Here are the markdown links with target titles for the provided text:</p>
<ol type="1">
<li><a href="https://al-tasmim.com/%D8%A3%D9%81%D8%B6%D9%84-%D8%B4%D8%B1%D9%83%D8%A9-%D8%AA%D9%88%D8%B5%D9%8A%D9%84-%D8%A7%D9%84%D8%B5%D8%B1%D9%81-%D8%A7%D9%84%D8%B5%D8%AD%D9%8A-%D8%A8%D9%85%D9%83%D8%A9/">شركة توصيل صرف صحي بمكة</a></li>
<li><a href="https://al-tasmim.com/%D8%B4%D8%B1%D9%83%D8%A9-%D8%B9%D8%B2%D9%84-0507976671-%D8%A3%D8%B3%D8%B7%D8%AD-%D8%A8%D9%85%D9%83%D8%A9/">شركة عزل اسطح بمكة</a></li>
<li><a href="https://al-tasmim.com/%D8%B4%D8%B1%D9%83%D9%87-%D8%B9%D8%B2%D9%84-%D8%AE%D8%B2%D8%A7%D9%86%D8%A7%D8%AA-%D8%A7%D9%84%D9%85%D9%8A%D8%A7%D8%A9-%D8%A8%D9%85%D9%83%D8%A9/">عزل خزانات بمكة</a></li>
<li><a href="https://al-tasmim.com/%D8%B4%D8%B1%D9%83%D8%A9-%D9%84%D8%AD%D8%A7%D9%85-%D8%AE%D8%B2%D8%A7%D9%86%D8%A7%D8%AA-%D9%81%D9%8A%D8%A8%D8%B1-%D8%AC%D9%84%D8%A7%D8%B3-%D8%A8%D9%85%D9%83%D8%A9/">شركة لحام خزانات بمكة</a></li>
<li><a href="https://al-tasmim.com/%D8%A3%D9%81%D8%B6%D9%84-%D8%B4%D8%B1%D9%83%D8%A9-%D8%AA%D9%85%D8%AF%D9%8A%D8%AF-%D8%A7%D9%84%D8%BA%D8%A7%D8%B2-%D9%81%D9%89-%D9%85%D9%83%D8%A9/">شركة تمديد غاز بمكة</a></li>
</ol>
<p>In the ever-evolving landscape of SEO, the integration of NLP marks a paradigm shift towards more sophisticated and nuanced optimization strategies. By harnessing the power of NLP, we not only decipher the language of search but also anticipate and cater to the evolving needs of our audience. As we continue to refine our approach and explore new frontiers, the synergy between NLP and SEO promises to redefine the digital landscape, empowering businesses to thrive in an era of unparalleled connectivity and discovery. you can read more articles <a href="https://kareemai.com/blog/feed.html">here</a></p>


</section>

 ]]></description>
  <category>life</category>
  <category>blogging</category>
  <category>publish</category>
  <category>seo</category>
  <guid>https://kareemai.com/blog/posts/seo/how_i_use_nlp_for_seo.html</guid>
  <pubDate>Mon, 17 Jul 2023 21:00:00 GMT</pubDate>
</item>
<item>
  <title>why i am blogging</title>
  <dc:creator>Kareem Elkhateb</dc:creator>
  <link>https://kareemai.com/blog/posts/life_style/why_I_am_blogging.html</link>
  <description><![CDATA[ 




<section id="table-of-contents" class="level2">
<h2 class="anchored" data-anchor-id="table-of-contents">Table of contents</h2>
</section>
<section id="why-i-am-blogging" class="level2">
<h2 class="anchored" data-anchor-id="why-i-am-blogging">Why I am blogging</h2>
<p>Here is the revised version of your blog with corrected grammar:</p>
<ol type="1">
<li>I am blogging to establish a presence on the internet and hope to collaborate with people who are interested in what I write or think about (reducing the search space by reversing the process).</li>
<li>It serves as a resume and blueprint for my existence in this world.</li>
<li>My goal is to help others by providing valuable content that makes their lives easier and better.</li>
<li>Organizing my knowledge and recapping topics I’ve forgotten helps me experience “aha” moments! 💡
<ol type="1">
<li>It’s a test of whether I truly understand something or not.</li>
</ol></li>
<li>I don’t like social media posts, Facebook, Twitter, or LinkedIn articles. I believe these platforms are not the best places for technical blogs, and their disadvantages outweigh the advantages.</li>
<li>I value the freedom of speech. Some topics or thoughts may be censored on social media, but I want to express my opinions without restrictions from others whose opinions I may not respect.</li>
<li>I want to discuss topics while I’m learning them because they’re still fresh in my mind. This will help anyone in a similar position as me in the future.</li>
<li>When studying a topic and knowing that I can write about it on my blog, I find my mind more focused on details that would fit well in the blog. This makes the learning process more enjoyable, focused, and valuable.</li>
<li>I want my posts and knowledge to be more organized!
<ol type="1">
<li>Who will search for a post written six years ago on Facebook or see your first tweet?</li>
<li>This problem is related to the feed of these social platforms.</li>
</ol></li>
</ol>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>https://medium.com/<span class="citation" data-cites="racheltho/why-you-yes-you-should-blog-7d2544ac1045">@racheltho/why-you-yes-you-should-blog-7d2544ac1045</span></li>
</ul>


</section>

 ]]></description>
  <category>life</category>
  <category>blogging</category>
  <category>publish</category>
  <guid>https://kareemai.com/blog/posts/life_style/why_I_am_blogging.html</guid>
  <pubDate>Mon, 17 Jul 2023 21:00:00 GMT</pubDate>
</item>
<item>
  <title>havrard CS197 AI research experiences</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/blog/posts/courses/havrard CS197 AI research experiences.html</link>
  <description><![CDATA[ 




<section id="table-of-contents" class="level2">
<h2 class="anchored" data-anchor-id="table-of-contents">Table of contents</h2>
<ul class="task-list">
<li><label><input type="checkbox">This course consists of 21 quick lectures that include valuable experiences and important tips for anyone interested in the field of scientific research, especially deep learning. The course can be completed in approximately one or two days. The title is not very precise, and the content of the lectures ranges from about 8 to 26 pages each. Some topics may not be directly related to the specifics of scientific research, but they are generally very helpful lectures for beginners in the field.</label></li>
</ul>
</section>
<section id="reviews" class="level2">
<h2 class="anchored" data-anchor-id="reviews">Reviews</h2>
<p>Lecture 1: Exciting Advances with AI Language Models <strong>Content</strong> : Interact with language models like GPT-3’s text completion and use Codex’s code generation abilities <strong>feedback</strong> : ⭐ (1/5)</p>
<hr>
<p>Lecture 2: The Zen of python <strong>Content</strong> : vscode,git,conad,linting and Debugging. <strong>feedback</strong>: <strong>feedback</strong> : ⭐ (1/5)</p>
<hr>
<p>Lecture 3: Reading AI Research papers <strong>Content</strong> :</p>
<ol type="1">
<li>Conduct a literature search to identify papers relevant to a topic of interest</li>
<li>Difference between Reading Wide and Reading deep and how to balance between them</li>
<li>How to use Google Scholar and paper with code <strong>feedback</strong> : ⭐⭐⭐⭐⭐ (5/5)</li>
</ol>
<hr>
<p>Lecture 4: In-Tune with Jazz Hands <strong>Content</strong>:</p>
<ol type="1">
<li>quick intro into huggingface</li>
<li>Tokenization</li>
<li>Causal language modeling (CLM) <strong>feedback</strong> : ⭐⭐⭐⭐ (4/5)</li>
</ol>
<hr>
<p>Lecture 5: Lightning McTorch <strong>Content</strong> :</p>
<ol type="1">
<li>Fine-tuning A vision Transformer</li>
<li>Intro to pytorch lightning (Lightning)</li>
<li>Data Loading</li>
<li>How to Build a Neural net Module with lightning and how lightning modules work <strong>feedback</strong> : ⭐⭐⭐⭐ (4/5)</li>
</ol>
<hr>
<p>Lecture 6 &amp; 7: Moonwalking with Pytorch <strong>Content</strong> :</p>
<ol type="1">
<li>Pytorch Exercises</li>
<li>Tensors</li>
<li>Autograd and neural networks <strong>feedback</strong> : ⭐ (1/5)</li>
</ol>
<hr>
<p>Lecture 8 &amp; 9: Experiment Organization Spakrs Joy <strong>Content</strong> :</p>
<ol type="1">
<li>Weight and Biases</li>
<li>Hyperparameter Search</li>
<li>Hydra <strong>feedback</strong> : ⭐⭐⭐⭐ (4/5)</li>
</ol>
<hr>
<p>Lecture 10 &amp; 11 : I Dreamed a Dream <strong>Content</strong></p>
<ol type="1">
<li>Identifying Gaps in A Research Paper
<ol type="1">
<li>CLIP and CheXzero</li>
</ol></li>
<li>Generating Ideas for Building a Research Paper</li>
<li>Iterating on your research ideas <strong>feedback</strong> : ⭐⭐⭐⭐⭐ (5/5)</li>
</ol>
<hr>
<p>Lecture 12 &amp; 13 : Today Was a Fairytale</p>
<ol type="1">
<li>how to deconstruct the elements of a research paper and their sequence</li>
<li>Resulting template that you can use as a general example <strong>feedback</strong> : ⭐⭐⭐⭐ (4/5)</li>
</ol>
<hr>
<p>Lecture 14 &amp; 15: Deep Learning on Cloud Nine <em>didn’t complete it</em> 🙃🙃🙃</p>
<hr>
<p>Lecture 16 &amp; 17:Make your dreams come tuned <strong>Content</strong></p>
<ol type="1">
<li>high level use of Stable Diffusion using a Dreambooth template</li>
<li>Use AWS to accelerate the training of Stable Diffusion models with GPUs</li>
<li>HF Accelerator <strong>feedback</strong> : ⭐⭐ (2/5)</li>
</ol>
<hr>
<p>Lecture 18 : Research Productivity Power-Ups <strong>Content</strong></p>
<ol type="1">
<li>How update meetings and working sessions</li>
<li>organizing your efforts on a project</li>
<li>what is technical dept and examples on it <strong>feedback</strong> : ⭐⭐⭐⭐ (4/5)</li>
</ol>
<hr>
<p>Lecture 19 :The AI Ninja <strong>Content</strong></p>
<ol type="1">
<li>How to make Steady Progress</li>
<li>Some Research Skills</li>
<li>Discussion Questions <strong>feedback</strong> : ⭐⭐ (2/5) I found that <a href="https://colah.github.io/notes/taste/">Colah’ blog</a> content about research is better in the context and offers a great details</li>
</ol>
<hr>
<p>Lecture 20: Bejeweled ⭐⭐⭐⭐⭐(5/5)</p>
<ol type="1">
<li>how to make a slides to improve your research talk</li>
<li>Assertion Evidence Approach <strong>feedback</strong> : ⭐⭐ (2/5) This is great related talk from MIT about this topic <a href="https://www.youtube.com/watch?v=Unzc731iCUY&amp;pp=ygUyaG93IHRvIG1ha2Ugc2xpZGVzaG93IGFuZCBwcmVzZW50YXRpb24gbWl0IGxlY3R1cmU%3D">How to speak</a> ⭐⭐⭐⭐⭐(5/5)</li>
</ol>
<hr>
<p>Lecture 21 : Model Showdown <strong>Content</strong></p>
<ol type="1">
<li>Statistical Testing <strong>feedback</strong> : ⭐⭐ ⭐(3/5)</li>
</ol>
<hr>


</section>

 ]]></description>
  <guid>https://kareemai.com/blog/posts/courses/havrard CS197 AI research experiences.html</guid>
  <pubDate>Mon, 17 Jul 2023 21:00:00 GMT</pubDate>
</item>
</channel>
</rss>
